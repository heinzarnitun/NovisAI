{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ff59fd-8cb3-49f2-b7d8-04f8b429b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded dataset: (209, 42)\n",
      "[INFO] Original Class Distribution:\n",
      "Career_Interest\n",
      "1    64\n",
      "0    53\n",
      "3    51\n",
      "2    41\n",
      "Name: count, dtype: int64\n",
      "[INFO] Selected Features via ANOVA (<0.05):\n",
      "['Gender', 'Math', 'Bio', 'Chemistry', 'Physics', 'Business', 'Average_Score', 'Study_Method', 'English_Proficiency', 'Favorite_Subject', 'Building miniatures / models', 'Hackathons / App development projects', 'Programming / coding clubs', 'Volunteering at hospitals, clinics, or NGOs', 'Model building recognition', 'Programming / Coding award']\n",
      "[INFO] Training Class Distribution:\n",
      "Career_Interest\n",
      "1    51\n",
      "0    42\n",
      "3    41\n",
      "2    33\n",
      "Name: count, dtype: int64\n",
      "[INFO] Testing Class Distribution:\n",
      "Career_Interest\n",
      "1    13\n",
      "0    11\n",
      "3    10\n",
      "2     8\n",
      "Name: count, dtype: int64\n",
      "[INFO] After SMOTE/Bootstrap Class Distribution:\n",
      "Career_Interest\n",
      "2    250\n",
      "1    250\n",
      "0    250\n",
      "3    250\n",
      "Name: count, dtype: int64\n",
      "[INFO] Fold 1 Training...\n",
      "[INFO] Fold 2 Training...\n",
      "[INFO] Fold 3 Training...\n",
      "[INFO] Fold 4 Training...\n",
      "[INFO] Fold 5 Training...\n",
      "\n",
      "============ AVERAGE METRICS ACROSS 5-FOLDS ============\n",
      "RF   - Precision: 0.641, Recall: 0.619, F1: 0.622, Accuracy: 0.624\n",
      "XGB  - Precision: 0.611, Recall: 0.578, F1: 0.582, Accuracy: 0.581\n",
      "Ensemble - Precision: 0.622, Recall: 0.591, F1: 0.596, Accuracy: 0.595\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import copy\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ Load & Clean Dataset\n",
    "# =====================================================\n",
    "df = pd.read_csv(\"survey_data_cleaned.csv\")\n",
    "drop_cols = ['Timestamp', 'Date_Of_Birth', 'Age']\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "print(\"[INFO] Loaded dataset:\", df.shape)\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ Split Target / Features\n",
    "# =====================================================\n",
    "target = \"Career_Interest\"\n",
    "X_raw = df.drop(columns=[target])\n",
    "y_raw = df[target]\n",
    "\n",
    "print(\"[INFO] Original Class Distribution:\")\n",
    "print(y_raw.value_counts())\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ Encode Categorical Columns\n",
    "# =====================================================\n",
    "label_encoders = {}\n",
    "X = X_raw.copy()\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "y = y_raw.copy()\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ ANOVA Feature Selection\n",
    "# =====================================================\n",
    "F_vals, p_vals = f_classif(X, y)\n",
    "anova_df = pd.DataFrame({\"Feature\": X.columns, \"F_value\": F_vals, \"p_value\": p_vals})\n",
    "selected_features = anova_df[anova_df[\"p_value\"] < 0.05][\"Feature\"].tolist()\n",
    "X_sel = X[selected_features]\n",
    "\n",
    "print(\"[INFO] Selected Features via ANOVA (<0.05):\")\n",
    "print(selected_features)\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ Train-Test Split (80% Train / 20% Test)\n",
    "# =====================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sel, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"[INFO] Training Class Distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"[INFO] Testing Class Distribution:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ Scaling\n",
    "# =====================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ SMOTE / Bootstrap Expansion (~1000 samples)\n",
    "# =====================================================\n",
    "n_classes = len(np.unique(y_train))\n",
    "n_target_per_class = 1000 // n_classes  # ~250 per class if 4 classes\n",
    "sm = SMOTE(sampling_strategy={cls: n_target_per_class for cls in np.unique(y_train)}, random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"[INFO] After SMOTE/Bootstrap Class Distribution:\")\n",
    "print(pd.Series(y_res).value_counts())\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ 5-Fold Cross-Validation on Bootstrapped Training Set\n",
    "# =====================================================\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "metrics_rf, metrics_xgb, metrics_ens = [], [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_res, y_res), 1):\n",
    "    print(f\"[INFO] Fold {fold} Training...\")\n",
    "\n",
    "    X_fold_train, X_fold_val = X_res[train_idx], X_res[val_idx]\n",
    "    y_fold_train, y_fold_val = y_res[train_idx], y_res[val_idx]\n",
    "\n",
    "    # Internal Bootstrapping\n",
    "    boot_idx = np.random.choice(len(X_fold_train), len(X_fold_train), replace=True)\n",
    "    X_fold_train_boot = X_fold_train[boot_idx]\n",
    "    y_fold_train_boot = y_fold_train.iloc[boot_idx]\n",
    "\n",
    "\n",
    "    # Train Models\n",
    "    rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42\n",
    "    )\n",
    "    ens = VotingClassifier(\n",
    "        estimators=[(\"rf\", rf), (\"xgb\", xgb)],\n",
    "        voting=\"soft\"\n",
    "    )\n",
    "\n",
    "    rf.fit(X_fold_train_boot, y_fold_train_boot)\n",
    "    xgb.fit(X_fold_train_boot, y_fold_train_boot)\n",
    "    ens.fit(X_fold_train_boot, y_fold_train_boot)\n",
    "\n",
    "    # Evaluate on Original 20% Test Set\n",
    "    for model, metric_list in zip([rf, xgb, ens], [metrics_rf, metrics_xgb, metrics_ens]):\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        metric_list.append([\n",
    "            precision_score(y_test, y_pred, average='macro'),\n",
    "            recall_score(y_test, y_pred, average='macro'),\n",
    "            f1_score(y_test, y_pred, average='macro'),\n",
    "            accuracy_score(y_test, y_pred)\n",
    "        ])\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ Average Metrics\n",
    "# =====================================================\n",
    "def avg_metrics(metrics_list):\n",
    "    return np.mean(np.array(metrics_list), axis=0)\n",
    "\n",
    "avg_rf = avg_metrics(metrics_rf)\n",
    "avg_xgb = avg_metrics(metrics_xgb)\n",
    "avg_ens = avg_metrics(metrics_ens)\n",
    "\n",
    "print(\"\\n============ AVERAGE METRICS ACROSS 5-FOLDS ============\")\n",
    "print(\"RF   - Precision: {:.3f}, Recall: {:.3f}, F1: {:.3f}, Accuracy: {:.3f}\".format(*avg_rf))\n",
    "print(\"XGB  - Precision: {:.3f}, Recall: {:.3f}, F1: {:.3f}, Accuracy: {:.3f}\".format(*avg_xgb))\n",
    "print(\"Ensemble - Precision: {:.3f}, Recall: {:.3f}, F1: {:.3f}, Accuracy: {:.3f}\".format(*avg_ens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ba56b-27e5-417a-abca-42ee14bebebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
