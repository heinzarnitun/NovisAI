{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA"
      ],
      "metadata": {
        "id": "FUBRQMBp6s2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "AUGMENTATION_FACTOR = 3\n",
        "\n",
        "# --------------------------------------\n",
        "# 1) Load & Basic Cleaning\n",
        "# --------------------------------------\n",
        "df = pd.read_csv(\"survey_data_cleaned.csv\")\n",
        "drop_cols = ['Timestamp', 'Date_Of_Birth', 'Age']\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
        "\n",
        "target_col = \"Career_Interest\"\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(str)\n",
        "\n",
        "print(\"[INFO] Original shape:\", df.shape)\n",
        "print(\"[INFO] Original class counts:\\n\", y.value_counts(), \"\\n\")\n",
        "\n",
        "# --------------------------------------\n",
        "# 2) Feature Type Detection\n",
        "# --------------------------------------\n",
        "numeric_continuous = ['Math','English','Bio','Chemistry','Physics','ICT',\n",
        "                      'Business','Average_Score','IELTS_Score']\n",
        "\n",
        "ordinal_features = ['Parents_Education_Encoded','Family_Lifestyle_Encoded']\n",
        "\n",
        "binary_features = [\n",
        "    'Building miniatures / models','DIY projects','Debate / Public Speaking',\n",
        "    'Hackathons / App development projects','Programming / coding clubs',\n",
        "    'Science clubs','Student Council / Leadership Roles',\n",
        "    'Volunteering at hospitals, clinics, or NGOs','DIY / Project recognition',\n",
        "    'Debate / Public Speaking awards','Hackathon / App competition prize',\n",
        "    'Model building recognition','Other (please specify)',\n",
        "    'Programming / Coding award','Science fair / Olympiad prize',\n",
        "    'Student Council / Leadership role','Volunteering recognition'\n",
        "]\n",
        "\n",
        "categorical_object = [\n",
        "    'Gender','Age_Group','Aptitude','if_HS_Student','Study_Method','Study_Habit',\n",
        "    'English_Proficiency','Favorite_Subject','Personality_Trait','Address',\n",
        "    'Study_Country','Chosen_University','Influence'\n",
        "]\n",
        "\n",
        "print(\"[INFO] Continuous numeric:\", numeric_continuous)\n",
        "print(\"[INFO] Ordinal:\", ordinal_features)\n",
        "print(\"[INFO] Binary:\", binary_features)\n",
        "print(\"[INFO] Categorical:\", categorical_object, \"\\n\")\n",
        "\n",
        "# --------------------------------------\n",
        "# 3) Encode categorical (object only)\n",
        "# --------------------------------------\n",
        "X_enc = X.copy()\n",
        "encoders = {}\n",
        "for col in categorical_object:\n",
        "    le = LabelEncoder()\n",
        "    X_enc[col] = le.fit_transform(X_enc[col].astype(str))\n",
        "    encoders[col] = le\n",
        "\n",
        "# (binary + ordinal already numeric — keep them as-is)\n",
        "\n",
        "# Encode target\n",
        "le_target = LabelEncoder()\n",
        "y_enc = le_target.fit_transform(y)\n",
        "X_enc[target_col] = y_enc\n",
        "\n",
        "# --------------------------------------\n",
        "# 4) Gaussian Noise ONLY on continuous features\n",
        "# --------------------------------------\n",
        "def augment_numeric_only(df_original, factor, target_col, noise_std=0.05):\n",
        "    df_list = [df_original.copy()]\n",
        "    for _ in range(factor-1):\n",
        "        df_new = df_original.copy()\n",
        "        for col in numeric_continuous:\n",
        "            std = df_original[col].std()\n",
        "            noise = np.random.normal(0, noise_std * std, size=len(df_original))\n",
        "            df_new[col] = np.maximum(df_new[col] + noise, 0)\n",
        "        df_list.append(df_new)\n",
        "    return pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df_augmented = augment_numeric_only(X_enc, AUGMENTATION_FACTOR, target_col)\n",
        "print(\"[INFO] Augmented dataset size:\", len(df_augmented), \"\\n\")\n",
        "\n",
        "X_aug = df_augmented.drop(columns=[target_col])\n",
        "y_aug = df_augmented[target_col]\n",
        "\n",
        "# --------------------------------------\n",
        "# 5) Split (Train/Valid/Test)\n",
        "# --------------------------------------\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_aug, y_aug, test_size=0.2, stratify=y_aug, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.25, stratify=y_train_temp, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"[INFO] Train/Valid/Test sizes:\", len(X_train), len(X_valid), len(X_test))\n",
        "\n",
        "# --------------------------------------\n",
        "# 6) SMOTE on train only\n",
        "# --------------------------------------\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"[INFO] After SMOTE:\", pd.Series(y_train_sm).value_counts(), \"\\n\")\n",
        "\n",
        "# --------------------------------------\n",
        "# 7) Scaling\n",
        "# --------------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --------------------------------------\n",
        "# 8) ANOVA (Feature Selection)\n",
        "# --------------------------------------\n",
        "F_vals, p_vals = f_classif(X_train_scaled, y_train_sm)\n",
        "anova_df = pd.DataFrame({'Feature': X_train.columns, 'F_value': F_vals, 'p_value': p_vals})\n",
        "\n",
        "selected_features = anova_df[anova_df['p_value'] < 0.05]['Feature'].tolist()\n",
        "if len(selected_features) == 0:\n",
        "    selected_features = X_train.columns.tolist()\n",
        "\n",
        "print(\"[INFO] ANOVA selected:\", selected_features, \"\\n\")\n",
        "print(\"[INFO] Number selected:\", len(selected_features), \"\\n\")\n",
        "\n",
        "X_train_sel = pd.DataFrame(X_train_scaled, columns=X_train.columns)[selected_features].values\n",
        "X_valid_sel = pd.DataFrame(X_valid_scaled, columns=X_valid.columns)[selected_features].values\n",
        "X_test_sel = pd.DataFrame(X_test_scaled, columns=X_test.columns)[selected_features].values\n",
        "\n",
        "# --------------------------------------\n",
        "# 9) Models\n",
        "# --------------------------------------\n",
        "models = {\n",
        "    'RF': RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
        "    'XGB': XGBClassifier(eval_metric='mlogloss', random_state=RANDOM_STATE),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(128,64), max_iter=300, random_state=RANDOM_STATE),\n",
        "    'LR': LogisticRegression(max_iter=500),\n",
        "    'SVM': SVC(probability=True, random_state=RANDOM_STATE),\n",
        "    'MLR': LinearRegression()  # Multiclass regression baseline\n",
        "}\n",
        "\n",
        "trained = {}\n",
        "for name, mdl in models.items():\n",
        "    mdl.fit(X_train_sel, y_train_sm)\n",
        "    trained[name] = mdl\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('RF', trained['RF']), ('XGB', trained['XGB'])],\n",
        "    voting='soft'\n",
        ")\n",
        "ensemble.fit(X_train_sel, y_train_sm)\n",
        "trained['ENSEMBLE'] = ensemble\n",
        "\n",
        "# --------------------------------------\n",
        "# 10) Evaluation\n",
        "# --------------------------------------\n",
        "def predict_mdl(name, model, X):\n",
        "    if name == 'MLR':\n",
        "        y_hat = np.rint(model.predict(X)).astype(int)\n",
        "        return np.clip(y_hat, y_enc.min(), y_enc.max())\n",
        "    return model.predict(X)\n",
        "\n",
        "print(\"\\n--- ACCURACY (Train / Valid / Test) ---\")\n",
        "for name, mdl in trained.items():\n",
        "    print(\n",
        "        name,\n",
        "        \"| Train:\", accuracy_score(y_train_sm, predict_mdl(name, mdl, X_train_sel)),\n",
        "        \"| Valid:\", accuracy_score(y_valid, predict_mdl(name, mdl, X_valid_sel)),\n",
        "        \"| Test:\", accuracy_score(y_test, predict_mdl(name, mdl, X_test_sel))\n",
        "    )\n",
        "\n",
        "print(\"\\n--- TEST SET CLASSIFICATION REPORTS ---\")\n",
        "for name, mdl in trained.items():\n",
        "    print(\"\\n======\", name, \"======\")\n",
        "    y_pred = predict_mdl(name, mdl, X_test_sel)\n",
        "    print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4DghoV337Kt",
        "outputId": "ff8adbe8-8db6-4be1-9905-9fe2cbcc66a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Original shape: (209, 42)\n",
            "[INFO] Original class counts:\n",
            " Career_Interest\n",
            "1    64\n",
            "0    53\n",
            "3    51\n",
            "2    41\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] Continuous numeric: ['Math', 'English', 'Bio', 'Chemistry', 'Physics', 'ICT', 'Business', 'Average_Score', 'IELTS_Score']\n",
            "[INFO] Ordinal: ['Parents_Education_Encoded', 'Family_Lifestyle_Encoded']\n",
            "[INFO] Binary: ['Building miniatures / models', 'DIY projects', 'Debate / Public Speaking', 'Hackathons / App development projects', 'Programming / coding clubs', 'Science clubs', 'Student Council / Leadership Roles', 'Volunteering at hospitals, clinics, or NGOs', 'DIY / Project recognition', 'Debate / Public Speaking awards', 'Hackathon / App competition prize', 'Model building recognition', 'Other (please specify)', 'Programming / Coding award', 'Science fair / Olympiad prize', 'Student Council / Leadership role', 'Volunteering recognition']\n",
            "[INFO] Categorical: ['Gender', 'Age_Group', 'Aptitude', 'if_HS_Student', 'Study_Method', 'Study_Habit', 'English_Proficiency', 'Favorite_Subject', 'Personality_Trait', 'Address', 'Study_Country', 'Chosen_University', 'Influence'] \n",
            "\n",
            "[INFO] Augmented dataset size: 627 \n",
            "\n",
            "[INFO] Train/Valid/Test sizes: 375 126 126\n",
            "[INFO] After SMOTE: Career_Interest\n",
            "3    115\n",
            "1    115\n",
            "2    115\n",
            "0    115\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] ANOVA selected: ['Gender', 'Age_Group', 'Aptitude', 'Parents_Education_Encoded', 'Family_Lifestyle_Encoded', 'Math', 'English', 'Bio', 'Chemistry', 'Physics', 'ICT', 'Business', 'Average_Score', 'Study_Method', 'English_Proficiency', 'IELTS_Score', 'Favorite_Subject', 'Address', 'Influence', 'Building miniatures / models', 'Hackathons / App development projects', 'Programming / coding clubs', 'Volunteering at hospitals, clinics, or NGOs', 'DIY / Project recognition', 'Debate / Public Speaking awards', 'Hackathon / App competition prize', 'Model building recognition', 'Other (please specify)', 'Programming / Coding award', 'Volunteering recognition'] \n",
            "\n",
            "[INFO] Number selected: 30 \n",
            "\n",
            "\n",
            "--- ACCURACY (Train / Valid / Test) ---\n",
            "RF | Train: 1.0 | Valid: 0.9682539682539683 | Test: 0.8809523809523809\n",
            "XGB | Train: 1.0 | Valid: 0.9603174603174603 | Test: 0.8888888888888888\n",
            "MLP | Train: 1.0 | Valid: 0.9761904761904762 | Test: 0.9047619047619048\n",
            "LR | Train: 0.8565217391304348 | Valid: 0.7777777777777778 | Test: 0.6507936507936508\n",
            "SVM | Train: 0.9804347826086957 | Valid: 0.9285714285714286 | Test: 0.8333333333333334\n",
            "MLR | Train: 0.5108695652173914 | Valid: 0.4365079365079365 | Test: 0.46825396825396826\n",
            "ENSEMBLE | Train: 1.0 | Valid: 0.9603174603174603 | Test: 0.8888888888888888\n",
            "\n",
            "--- TEST SET CLASSIFICATION REPORTS ---\n",
            "\n",
            "====== RF ======\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.75      0.81        32\n",
            "           1       0.91      0.84      0.88        38\n",
            "           2       0.86      1.00      0.93        25\n",
            "           3       0.86      0.97      0.91        31\n",
            "\n",
            "    accuracy                           0.88       126\n",
            "   macro avg       0.88      0.89      0.88       126\n",
            "weighted avg       0.88      0.88      0.88       126\n",
            "\n",
            "\n",
            "====== XGB ======\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84        32\n",
            "           1       0.91      0.84      0.88        38\n",
            "           2       0.93      1.00      0.96        25\n",
            "           3       0.85      0.94      0.89        31\n",
            "\n",
            "    accuracy                           0.89       126\n",
            "   macro avg       0.89      0.90      0.89       126\n",
            "weighted avg       0.89      0.89      0.89       126\n",
            "\n",
            "\n",
            "====== MLP ======\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.75      0.83        32\n",
            "           1       0.92      0.89      0.91        38\n",
            "           2       0.93      1.00      0.96        25\n",
            "           3       0.86      1.00      0.93        31\n",
            "\n",
            "    accuracy                           0.90       126\n",
            "   macro avg       0.91      0.91      0.91       126\n",
            "weighted avg       0.91      0.90      0.90       126\n",
            "\n",
            "\n",
            "====== LR ======\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.53      0.54        32\n",
            "           1       0.70      0.61      0.65        38\n",
            "           2       0.69      0.72      0.71        25\n",
            "           3       0.67      0.77      0.72        31\n",
            "\n",
            "    accuracy                           0.65       126\n",
            "   macro avg       0.65      0.66      0.65       126\n",
            "weighted avg       0.65      0.65      0.65       126\n",
            "\n",
            "\n",
            "====== SVM ======\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.75      0.76        32\n",
            "           1       0.90      0.68      0.78        38\n",
            "           2       0.86      0.96      0.91        25\n",
            "           3       0.82      1.00      0.90        31\n",
            "\n",
            "    accuracy                           0.83       126\n",
            "   macro avg       0.84      0.85      0.84       126\n",
            "weighted avg       0.84      0.83      0.83       126\n",
            "\n",
            "\n",
            "====== MLR ======\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.34      0.43        32\n",
            "           1       0.49      0.55      0.52        38\n",
            "           2       0.31      0.60      0.41        25\n",
            "           3       0.75      0.39      0.51        31\n",
            "\n",
            "    accuracy                           0.47       126\n",
            "   macro avg       0.53      0.47      0.47       126\n",
            "weighted avg       0.54      0.47      0.47       126\n",
            "\n",
            "\n",
            "====== ENSEMBLE ======\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84        32\n",
            "           1       0.91      0.84      0.88        38\n",
            "           2       0.93      1.00      0.96        25\n",
            "           3       0.85      0.94      0.89        31\n",
            "\n",
            "    accuracy                           0.89       126\n",
            "   macro avg       0.89      0.90      0.89       126\n",
            "weighted avg       0.89      0.89      0.89       126\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MI"
      ],
      "metadata": {
        "id": "vW3X2E2W6vnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import mutual_info_classif # ADDED for MI\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "AUGMENTATION_FACTOR = 3\n",
        "NOISE_STD_DEV = 0.1\n",
        "TOP_K_FEATURES = 30 # Defined a value for MI selection\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load & clean\n",
        "# ----------------------------\n",
        "df = pd.read_csv(\"survey_data_cleaned.csv\")\n",
        "drop_cols = ['Timestamp', 'Date_Of_Birth', 'Age']\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
        "\n",
        "target_col = 'Career_Interest'\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(str)\n",
        "\n",
        "print(\"[INFO] Original shape:\", df.shape)\n",
        "print(\"[INFO] Original class counts:\\n\", y.value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# Detect feature types automatically\n",
        "# ----------------------------\n",
        "all_features = X.columns.tolist()\n",
        "\n",
        "# numeric dtype columns (may include encoded categoricals and binary dummies)\n",
        "numeric_dtype_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# detect binary numeric columns (only 0/1)\n",
        "binary_features = []\n",
        "for c in numeric_dtype_cols:\n",
        "    vals = X[c].dropna().unique()\n",
        "    # consider values that are exactly integers 0/1 (allow float 0.0/1.0)\n",
        "    if set(np.unique(vals)).issubset({0, 1}):\n",
        "        binary_features.append(c)\n",
        "\n",
        "# continuous numeric: numeric dtype and reasonably many unique values\n",
        "continuous_numeric = []\n",
        "ordinal_encoded = []  # numeric but small set (e.g., 0/1/2)\n",
        "for c in numeric_dtype_cols:\n",
        "    if c in binary_features:\n",
        "        continue\n",
        "    nun = X[c].nunique(dropna=True)\n",
        "    if nun > 5:\n",
        "        continuous_numeric.append(c)\n",
        "    else:\n",
        "        # small number of unique values (<=5) treated as ordinal/categorical encoded\n",
        "        ordinal_encoded.append(c)\n",
        "\n",
        "# object dtype columns: true categorical strings -> we will label encode them\n",
        "object_categorical = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Remove any overlap\n",
        "for c in list(object_categorical):\n",
        "    if c in continuous_numeric: object_categorical.remove(c)\n",
        "for c in list(ordinal_encoded):\n",
        "    if c in object_categorical: ordinal_encoded.remove(c)\n",
        "\n",
        "print(\"[INFO] Detected continuous numeric features:\", continuous_numeric)\n",
        "print(\"[INFO] Detected ordinal/categorical-as-int features:\", ordinal_encoded)\n",
        "print(\"[INFO] Detected binary features:\", binary_features)\n",
        "print(\"[INFO] Detected object categorical features (to encode):\", object_categorical, \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Encode categorical features (only object/category ones)\n",
        "# ----------------------------\n",
        "X_enc = X.copy()\n",
        "encoders = {}\n",
        "\n",
        "# Label-encode object categorical columns (Address, Favorite_Subject, etc.)\n",
        "for col in object_categorical:\n",
        "    le = LabelEncoder()\n",
        "    # convert to str to handle NaNs uniformly\n",
        "    X_enc[col] = le.fit_transform(X_enc[col].astype(str))\n",
        "    encoders[col] = le\n",
        "\n",
        "# Keep ordinal_encoded as-is (they're already numeric, e.g., 0/1/2)\n",
        "# Keep binary_features as-is (0/1)\n",
        "\n",
        "# For completeness convert any remaining non-numeric to numeric safe\n",
        "for col in X_enc.columns:\n",
        "    if X_enc[col].dtype == 'object' or X_enc[col].dtype.name == 'category':\n",
        "        # fallback: try numeric conversion\n",
        "        X_enc[col] = pd.to_numeric(X_enc[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_enc = le_target.fit_transform(y)\n",
        "\n",
        "df_enc = X_enc.copy()\n",
        "df_enc[target_col] = y_enc\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Augmentation (Gaussian Noise) — ONLY on continuous numeric features\n",
        "# ----------------------------\n",
        "def create_augmented_data(df_original, factor, target_column, noise_std_dev=0.05):\n",
        "    \"\"\"\n",
        "    Add Gaussian noise only to continuous numeric columns (continuous_numeric).\n",
        "    Other columns (binary, ordinal, encoded categorical) are left unchanged.\n",
        "    \"\"\"\n",
        "    df_list = [df_original.copy()]\n",
        "    feature_cols = df_original.columns.drop(target_column).tolist()\n",
        "\n",
        "    # identify which of feature_cols are continuous numeric in current df\n",
        "    cont_cols = [c for c in feature_cols if c in continuous_numeric]\n",
        "\n",
        "    for _ in range(factor - 1):\n",
        "        df_new = df_original.copy()\n",
        "        # Add noise only to continuous columns\n",
        "        for col in cont_cols:\n",
        "            std = df_new[col].std()\n",
        "            # if std is NaN or zero, skip adding noise\n",
        "            if pd.isna(std) or std == 0:\n",
        "                continue\n",
        "            noise = np.random.normal(0, noise_std_dev * std, size=len(df_new))\n",
        "            df_new[col] = df_new[col] + noise\n",
        "            # optional: keep sensible bounds for scores (0-100) if column looks like a score\n",
        "            if col in ['Math', 'English', 'Bio', 'Chemistry', 'Physics', 'ICT', 'Business', 'Average_Score', 'IELTS_Score']:\n",
        "                df_new[col] = df_new[col].clip(lower=0)\n",
        "        # For non-continuous columns, keep values as-is (no rounding necessity)\n",
        "        df_list.append(df_new)\n",
        "\n",
        "    return pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df_augmented = create_augmented_data(df_enc, AUGMENTATION_FACTOR, target_col, NOISE_STD_DEV)\n",
        "print(f\"[INFO] Data augmented (GN on continuous numeric only). New total size: {df_augmented.shape[0]} samples.\")\n",
        "\n",
        "X_augmented = df_augmented.drop(columns=[target_col])\n",
        "y_augmented = df_augmented[target_col]\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Train/Validation/Test split (60/20/20)\n",
        "# ----------------------------\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_augmented, y_augmented, test_size=0.2, stratify=y_augmented, random_state=RANDOM_STATE\n",
        ")\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.25, stratify=y_train_temp, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Train Size: {len(X_train)} | Validation Size: {len(X_valid)} | Test Size: {len(X_test)}\")\n",
        "print(\"[INFO] Train class dist BEFORE SMOTE:\\n\", pd.Series(y_train).value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5) SMOTE on training data ONLY\n",
        "# ----------------------------\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "# SMOTE expects numeric array. All columns are numeric after label encoding; keep as-is.\n",
        "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"[INFO] Train class dist AFTER SMOTE:\\n\", pd.Series(y_train_sm).value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Scaling — ONLY continuous numeric features\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler on continuous numeric columns of X_train_sm\n",
        "# If a continuous column is missing after resampling (unlikely), handle gracefully\n",
        "cont_cols_present = [c for c in continuous_numeric if c in X_train_sm.columns]\n",
        "if cont_cols_present:\n",
        "    scaler.fit(X_train_sm[cont_cols_present])\n",
        "    # transform train, valid, test continuous columns\n",
        "    X_train_scaled_cont = pd.DataFrame(scaler.transform(X_train_sm[cont_cols_present]), columns=cont_cols_present, index=X_train_sm.index)\n",
        "    X_valid_scaled_cont = pd.DataFrame(scaler.transform(X_valid[cont_cols_present]), columns=cont_cols_present, index=X_valid.index)\n",
        "    X_test_scaled_cont = pd.DataFrame(scaler.transform(X_test[cont_cols_present]), columns=cont_cols_present, index=X_test.index)\n",
        "else:\n",
        "    # no continuous columns detected, create empty frames\n",
        "    X_train_scaled_cont = pd.DataFrame(index=X_train_sm.index)\n",
        "    X_valid_scaled_cont = pd.DataFrame(index=X_valid.index)\n",
        "    X_test_scaled_cont = pd.DataFrame(index=X_test.index)\n",
        "\n",
        "# Now build full scaled DataFrames by combining:\n",
        "# - scaled continuous numeric\n",
        "# - ordinal_encoded (leave as-is)\n",
        "# - binary_features (leave as-is)\n",
        "# - object categorical which were label-encoded earlier (present in X_train_sm)\n",
        "other_cols = [c for c in X_train_sm.columns if c not in cont_cols_present]\n",
        "\n",
        "# Convert index alignment and create DataFrames for other cols\n",
        "X_train_other = X_train_sm[other_cols].reset_index(drop=True)\n",
        "X_valid_other = X_valid.reset_index(drop=True)[other_cols]\n",
        "X_test_other = X_test.reset_index(drop=True)[other_cols]\n",
        "\n",
        "# Reset index for scaled continuous\n",
        "X_train_scaled_cont = X_train_scaled_cont.reset_index(drop=True)\n",
        "X_valid_scaled_cont = X_valid_scaled_cont.reset_index(drop=True)\n",
        "X_test_scaled_cont = X_test_scaled_cont.reset_index(drop=True)\n",
        "\n",
        "# Concatenate scaled continuous + other (order columns same as original X_train_sm)\n",
        "X_train_scaled = pd.concat([X_train_scaled_cont, X_train_other], axis=1)[X_train_sm.columns.tolist()]\n",
        "X_valid_scaled = pd.concat([X_valid_scaled_cont, X_valid_other], axis=1)[X_valid.columns.tolist()]\n",
        "X_test_scaled = pd.concat([X_test_scaled_cont, X_test_other], axis=1)[X_test.columns.tolist()]\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Mutual Information (MI) Feature Selection\n",
        "# ----------------------------\n",
        "print(\"\\n>>> [7/9] MUTUAL INFORMATION FEATURE SELECTION...\")\n",
        "# Calculate MI scores on the scaled, SMOTEd training data (all features numeric now)\n",
        "MI_scores = mutual_info_classif(X_train_scaled.values, y_train_sm, random_state=RANDOM_STATE)\n",
        "mi_df = pd.DataFrame({'Feature': X_train_scaled.columns, 'MI_Score': MI_scores})\n",
        "\n",
        "# Select top K features based on MI score\n",
        "selected_features = mi_df.nlargest(TOP_K_FEATURES, 'MI_Score')['Feature'].tolist()\n",
        "\n",
        "if len(selected_features) == 0:\n",
        "    print(\"[WARNING] MI selected 0 features. Using all features.\")\n",
        "    selected_features = X_train_scaled.columns.tolist()\n",
        "\n",
        "# Transform all datasets to selected features\n",
        "X_train_df = pd.DataFrame(X_train_scaled.values, columns=X_train_scaled.columns)\n",
        "X_valid_df = pd.DataFrame(X_valid_scaled.values, columns=X_valid.columns)\n",
        "X_test_df = pd.DataFrame(X_test_scaled.values, columns=X_test.columns)\n",
        "\n",
        "X_train_sel = X_train_df[selected_features].values\n",
        "X_valid_sel = X_valid_df[selected_features].values\n",
        "X_test_sel = X_test_df[selected_features].values\n",
        "\n",
        "print(f\"[INFO] MI selected {len(selected_features)} features: {selected_features}\")\n",
        "print(\"[INFO] Numbers of MI selected features:\", len(selected_features), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 8) Train models\n",
        "# -------------------\n",
        "models = {\n",
        "    'RF': RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
        "    'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_STATE),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(128,64), max_iter=300, random_state=RANDOM_STATE),\n",
        "    'LR': LogisticRegression(max_iter=500, random_state=RANDOM_STATE),\n",
        "    'SVM': SVC(probability=True, random_state=RANDOM_STATE),\n",
        "    'MLR': LinearRegression()\n",
        "}\n",
        "\n",
        "trained = {}\n",
        "for name, mdl in models.items():\n",
        "    print(f\"[INFO] Training {name} ...\")\n",
        "    mdl.fit(X_train_sel, y_train_sm)\n",
        "    trained[name] = mdl\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('RF', trained['RF']), ('XGB', trained['XGB'])],\n",
        "    voting='soft'\n",
        ")\n",
        "ensemble.fit(X_train_sel, y_train_sm)\n",
        "trained['ENSEMBLE'] = ensemble\n",
        "\n",
        "# ----------------------------\n",
        "# 9) Predict & evaluate\n",
        "# ----------------------------\n",
        "def predict_model(name, model, X):\n",
        "    if name == 'MLR':\n",
        "        y_hat = np.rint(model.predict(X)).astype(int)\n",
        "        y_hat = np.clip(y_hat, y_enc.min(), y_enc.max())\n",
        "        return y_hat\n",
        "    return model.predict(X)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\">>> EVALUATION: TRAINING, VALIDATION & TEST SETS <<<\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 9.1 Accuracy Comparison\n",
        "print(\"\\n--- ACCURACY COMPARISON with MI(Training vs. Validation vs. Test) ---\")\n",
        "for name, mdl in trained.items():\n",
        "    y_pred_train = predict_model(name, mdl, X_train_sel)\n",
        "    acc_train = accuracy_score(y_train_sm, y_pred_train)\n",
        "\n",
        "    y_pred_valid = predict_model(name, mdl, X_valid_sel)\n",
        "    acc_valid = accuracy_score(y_valid, y_pred_valid)\n",
        "\n",
        "    y_pred_test = predict_model(name, mdl, X_test_sel)\n",
        "    acc_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"   {name} Training: {acc_train:.4f} | Validation: {acc_valid:.4f} | Test: {acc_test:.4f}\")\n",
        "\n",
        "# 9.2 Full Classification Report for Test Set\n",
        "print(\"\\n--- FINAL TEST SET RESULTS (Unseen Data) ---\")\n",
        "for name, mdl in trained.items():\n",
        "    y_pred = predict_model(name, mdl, X_test_sel)\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    try:\n",
        "        print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n",
        "    except ValueError:\n",
        "        print(f\"Classification Report failed for {name} due to class mismatch. Showing only Accuracy.\")\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbfHI-3U5TMs",
        "outputId": "d0f33f6e-8377-452a-bb2c-4f7e671e914b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Original shape: (209, 42)\n",
            "[INFO] Original class counts:\n",
            " Career_Interest\n",
            "1    64\n",
            "0    53\n",
            "3    51\n",
            "2    41\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] Detected continuous numeric features: ['Math', 'English', 'Bio', 'Chemistry', 'Physics', 'ICT', 'Business', 'Average_Score', 'IELTS_Score']\n",
            "[INFO] Detected ordinal/categorical-as-int features: ['Family_Lifestyle_Encoded']\n",
            "[INFO] Detected binary features: ['Parents_Education_Encoded', 'Building miniatures / models', 'DIY projects', 'Debate / Public Speaking', 'Hackathons / App development projects', 'Programming / coding clubs', 'Science clubs', 'Student Council / Leadership Roles', 'Volunteering at hospitals, clinics, or NGOs', 'DIY / Project recognition', 'Debate / Public Speaking awards', 'Hackathon / App competition prize', 'Model building recognition', 'Other (please specify)', 'Programming / Coding award', 'Science fair / Olympiad prize', 'Student Council / Leadership role', 'Volunteering recognition']\n",
            "[INFO] Detected object categorical features (to encode): ['Gender', 'Age_Group', 'Aptitude', 'if_HS_Student', 'Study_Method', 'Study_Habit', 'English_Proficiency', 'Favorite_Subject', 'Personality_Trait', 'Address', 'Study_Country', 'Chosen_University', 'Influence'] \n",
            "\n",
            "[INFO] Data augmented (GN on continuous numeric only). New total size: 627 samples.\n",
            "[INFO] Train Size: 375 | Validation Size: 126 | Test Size: 126\n",
            "[INFO] Train class dist BEFORE SMOTE:\n",
            " Career_Interest\n",
            "1    115\n",
            "0     95\n",
            "3     91\n",
            "2     74\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] Train class dist AFTER SMOTE:\n",
            " Career_Interest\n",
            "3    115\n",
            "1    115\n",
            "2    115\n",
            "0    115\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "\n",
            ">>> [7/9] MUTUAL INFORMATION FEATURE SELECTION...\n",
            "[INFO] MI selected 30 features: ['Favorite_Subject', 'Chosen_University', 'Physics', 'Address', 'Average_Score', 'Gender', 'Bio', 'ICT', 'Math', 'English', 'English_Proficiency', 'Programming / coding clubs', 'IELTS_Score', 'Business', 'Chemistry', 'Building miniatures / models', 'Personality_Trait', 'Study_Country', 'Volunteering at hospitals, clinics, or NGOs', 'Hackathons / App development projects', 'Parents_Education_Encoded', 'Family_Lifestyle_Encoded', 'Study_Method', 'Age_Group', 'DIY projects', 'Student Council / Leadership Roles', 'Model building recognition', 'Debate / Public Speaking', 'Study_Habit', 'DIY / Project recognition']\n",
            "[INFO] Numbers of MI selected features: 30 \n",
            "\n",
            "[INFO] Training RF ...\n",
            "[INFO] Training XGB ...\n",
            "[INFO] Training MLP ...\n",
            "[INFO] Training LR ...\n",
            "[INFO] Training SVM ...\n",
            "[INFO] Training MLR ...\n",
            "\n",
            "============================================================\n",
            ">>> EVALUATION: TRAINING, VALIDATION & TEST SETS <<<\n",
            "============================================================\n",
            "\n",
            "--- ACCURACY COMPARISON with MI(Training vs. Validation vs. Test) ---\n",
            "   RF Training: 1.0000 | Validation: 0.9603 | Test: 0.8730\n",
            "   XGB Training: 1.0000 | Validation: 0.9365 | Test: 0.8492\n",
            "   MLP Training: 1.0000 | Validation: 0.9603 | Test: 0.8889\n",
            "   LR Training: 0.8196 | Validation: 0.7698 | Test: 0.6508\n",
            "   SVM Training: 0.3848 | Validation: 0.3333 | Test: 0.3333\n",
            "   MLR Training: 0.5000 | Validation: 0.4683 | Test: 0.4683\n",
            "   ENSEMBLE Training: 1.0000 | Validation: 0.9524 | Test: 0.8492\n",
            "\n",
            "--- FINAL TEST SET RESULTS (Unseen Data) ---\n",
            "\n",
            "===== RF =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.75      0.81        32\n",
            "           1       0.89      0.84      0.86        38\n",
            "           2       0.89      1.00      0.94        25\n",
            "           3       0.83      0.94      0.88        31\n",
            "\n",
            "    accuracy                           0.87       126\n",
            "   macro avg       0.87      0.88      0.88       126\n",
            "weighted avg       0.87      0.87      0.87       126\n",
            "\n",
            "Test Accuracy: 0.873015873015873\n",
            "\n",
            "===== XGB =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.72      0.78        32\n",
            "           1       0.80      0.84      0.82        38\n",
            "           2       0.96      1.00      0.98        25\n",
            "           3       0.82      0.87      0.84        31\n",
            "\n",
            "    accuracy                           0.85       126\n",
            "   macro avg       0.86      0.86      0.86       126\n",
            "weighted avg       0.85      0.85      0.85       126\n",
            "\n",
            "Test Accuracy: 0.8492063492063492\n",
            "\n",
            "===== MLP =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.75      0.83        32\n",
            "           1       0.86      0.84      0.85        38\n",
            "           2       1.00      1.00      1.00        25\n",
            "           3       0.82      1.00      0.90        31\n",
            "\n",
            "    accuracy                           0.89       126\n",
            "   macro avg       0.90      0.90      0.89       126\n",
            "weighted avg       0.89      0.89      0.89       126\n",
            "\n",
            "Test Accuracy: 0.8888888888888888\n",
            "\n",
            "===== LR =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.50      0.53        32\n",
            "           1       0.63      0.58      0.60        38\n",
            "           2       0.76      0.76      0.76        25\n",
            "           3       0.66      0.81      0.72        31\n",
            "\n",
            "    accuracy                           0.65       126\n",
            "   macro avg       0.65      0.66      0.66       126\n",
            "weighted avg       0.65      0.65      0.65       126\n",
            "\n",
            "Test Accuracy: 0.6507936507936508\n",
            "\n",
            "===== SVM =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.09      0.15        32\n",
            "           1       0.28      0.45      0.34        38\n",
            "           2       0.29      0.28      0.29        25\n",
            "           3       0.45      0.48      0.47        31\n",
            "\n",
            "    accuracy                           0.33       126\n",
            "   macro avg       0.35      0.33      0.31       126\n",
            "weighted avg       0.35      0.33      0.31       126\n",
            "\n",
            "Test Accuracy: 0.3333333333333333\n",
            "\n",
            "===== MLR =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.25      0.35        32\n",
            "           1       0.48      0.63      0.55        38\n",
            "           2       0.34      0.60      0.43        25\n",
            "           3       0.67      0.39      0.49        31\n",
            "\n",
            "    accuracy                           0.47       126\n",
            "   macro avg       0.51      0.47      0.45       126\n",
            "weighted avg       0.52      0.47      0.46       126\n",
            "\n",
            "Test Accuracy: 0.46825396825396826\n",
            "\n",
            "===== ENSEMBLE =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.72      0.78        32\n",
            "           1       0.80      0.84      0.82        38\n",
            "           2       0.96      1.00      0.98        25\n",
            "           3       0.82      0.87      0.84        31\n",
            "\n",
            "    accuracy                           0.85       126\n",
            "   macro avg       0.86      0.86      0.86       126\n",
            "weighted avg       0.85      0.85      0.85       126\n",
            "\n",
            "Test Accuracy: 0.8492063492063492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CV"
      ],
      "metadata": {
        "id": "Oo5TGTv9Cfx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "# Import both feature selection methods\n",
        "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
        "# Import for K-Fold splitting\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "AUGMENTATION_FACTOR = 3\n",
        "NOISE_STD_DEV = 0.1\n",
        "TOP_K_FEATURES = 30\n",
        "N_SPLITS = 5 # K for K-Fold Cross-Validation\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load & clean\n",
        "# ----------------------------\n",
        "df = pd.read_csv(\"survey_data_cleaned.csv\")\n",
        "drop_cols = ['Timestamp', 'Date_Of_Birth', 'Age']\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
        "\n",
        "target_col = 'Career_Interest'\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(str)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Encode categorical features\n",
        "# ----------------------------\n",
        "X_enc = X.copy()\n",
        "for col in X_enc.columns:\n",
        "    if X_enc[col].dtype == 'object' or X_enc[col].dtype.name == 'category':\n",
        "        le = LabelEncoder()\n",
        "        X_enc[col] = le.fit_transform(X_enc[col].astype(str))\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_enc = le_target.fit_transform(y)\n",
        "\n",
        "df_enc = X_enc.copy()\n",
        "df_enc[target_col] = y_enc\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Augmentation (Gaussian Noise)\n",
        "# ----------------------------\n",
        "def create_augmented_data(df_original, factor, target_column, noise_std_dev=0.05):\n",
        "    df_list = [df_original.copy()]\n",
        "    feature_cols = df_original.columns.drop(target_column).tolist()\n",
        "\n",
        "    for _ in range(factor - 1):\n",
        "        df_new = df_original.copy()\n",
        "        for col in feature_cols:\n",
        "            noise = np.random.normal(0, noise_std_dev * df_new[col].std(), size=len(df_new))\n",
        "            df_new[col] = np.maximum(df_new[col] + noise, 0)\n",
        "        df_list.append(df_new)\n",
        "\n",
        "    return pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df_augmented = create_augmented_data(df_enc, AUGMENTATION_FACTOR, target_col, NOISE_STD_DEV)\n",
        "print(f\"[INFO] Data augmented (GN). New total size: {df_augmented.shape[0]} samples.\")\n",
        "\n",
        "X_augmented = df_augmented.drop(columns=[target_col])\n",
        "y_augmented = df_augmented[target_col]\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Train/Test split (80% for CV/Training, 20% for Final Test)\n",
        "# ----------------------------\n",
        "# X_train_temp is the pool for Cross-Validation\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_augmented, y_augmented, test_size=0.2, stratify=y_augmented, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"[INFO] CV Pool Size: {len(X_train_temp)} | Final Test Size: {len(X_test)}\")\n",
        "\n",
        "# Convert X_train_temp to DataFrame to preserve column names\n",
        "X_train_temp = pd.DataFrame(X_train_temp, columns=X_augmented.columns)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Define Models for CV (Simplified to RF, MLP, LogReg for speed)\n",
        "# ----------------------------\n",
        "cv_models = {\n",
        "    'RF': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=RANDOM_STATE),\n",
        "    'LR': LogisticRegression(max_iter=500, random_state=RANDOM_STATE),\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6) Pipeline 1: ANOVA + K-Fold Cross-Validation\n",
        "# ----------------------------------------------------\n",
        "def run_cv_pipeline_anova(X, y, models, n_splits=N_SPLITS, top_k=TOP_K_FEATURES):\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    cv_results = {name: [] for name in models.keys()}\n",
        "\n",
        "    print(f\"\\n--- Running ANOVA Pipeline ({n_splits} Folds) ---\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train_f, X_val_f = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_f, y_val_f = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # 6.1) SMOTE on Training Fold ONLY (Prevents Data Leakage)\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_train_sm, y_train_sm = sm.fit_resample(X_train_f, y_train_f)\n",
        "\n",
        "        # 6.2) Scaling\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "        X_val_scaled = scaler.transform(X_val_f)\n",
        "\n",
        "        # 6.3) ANOVA Feature Selection\n",
        "        F_vals, p_vals = f_classif(X_train_scaled, y_train_sm)\n",
        "        anova_df = pd.DataFrame({'Feature': X.columns, 'p_value': p_vals})\n",
        "        selected_features = anova_df[anova_df['p_value'] < 0.05]['Feature'].tolist()\n",
        "        if len(selected_features) == 0:\n",
        "            selected_features = X.columns.tolist()\n",
        "\n",
        "        # 6.4) Apply Feature Selection\n",
        "        X_train_sel = pd.DataFrame(X_train_scaled, columns=X.columns)[selected_features].values\n",
        "        X_val_sel = pd.DataFrame(X_val_scaled, columns=X.columns)[selected_features].values\n",
        "\n",
        "        # 6.5) Train and Evaluate Models\n",
        "        for name, mdl in models.items():\n",
        "            mdl.fit(X_train_sel, y_train_sm)\n",
        "            y_pred = mdl.predict(X_val_sel)\n",
        "            acc = accuracy_score(y_val_f, y_pred)\n",
        "            cv_results[name].append(acc)\n",
        "\n",
        "    return {name: np.mean(scores) for name, scores in cv_results.items()}, selected_features\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 7) Pipeline 2: Mutual Information + K-Fold Cross-Validation\n",
        "# ----------------------------------------------------\n",
        "def run_cv_pipeline_mi(X, y, models, n_splits=N_SPLITS, top_k=TOP_K_FEATURES):\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    cv_results = {name: [] for name in models.keys()}\n",
        "\n",
        "    print(f\"\\n--- Running MI Pipeline ({n_splits} Folds) ---\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train_f, X_val_f = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_f, y_val_f = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # 7.1) SMOTE on Training Fold ONLY (Prevents Data Leakage)\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_train_sm, y_train_sm = sm.fit_resample(X_train_f, y_train_f)\n",
        "\n",
        "        # 7.2) Scaling\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "        X_val_scaled = scaler.transform(X_val_f)\n",
        "\n",
        "        # 7.3) Mutual Information Feature Selection\n",
        "        MI_scores = mutual_info_classif(X_train_scaled, y_train_sm, random_state=RANDOM_STATE)\n",
        "        mi_df = pd.DataFrame({'Feature': X.columns, 'MI_Score': MI_scores})\n",
        "        selected_features = mi_df.nlargest(top_k, 'MI_Score')['Feature'].tolist()\n",
        "        if len(selected_features) == 0:\n",
        "            selected_features = X.columns.tolist()\n",
        "\n",
        "        # 7.4) Apply Feature Selection\n",
        "        X_train_sel = pd.DataFrame(X_train_scaled, columns=X.columns)[selected_features].values\n",
        "        X_val_sel = pd.DataFrame(X_val_scaled, columns=X.columns)[selected_features].values\n",
        "\n",
        "        # 7.5) Train and Evaluate Models\n",
        "        for name, mdl in models.items():\n",
        "            mdl.fit(X_train_sel, y_train_sm)\n",
        "            y_pred = mdl.predict(X_val_sel)\n",
        "            acc = accuracy_score(y_val_f, y_pred)\n",
        "            cv_results[name].append(acc)\n",
        "\n",
        "    return {name: np.mean(scores) for name, scores in cv_results.items()}, selected_features\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 8) EXECUTION AND FINAL COMPARISON\n",
        "# ----------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RUNNING CROSS-VALIDATION PIPELINES (Requires IMBLEARN and XGBOOST)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run ANOVA Pipeline\n",
        "cv_anova_scores, anova_features = run_cv_pipeline_anova(X_train_temp, y_train_temp, cv_models.copy())\n",
        "print(\"ANOVA CV Results (Mean Accuracy):\", cv_anova_scores)\n",
        "\n",
        "# Run MI Pipeline\n",
        "cv_mi_scores, mi_features = run_cv_pipeline_mi(X_train_temp, y_train_temp, cv_models.copy())\n",
        "print(\"MI CV Results (Mean Accuracy):\", cv_mi_scores)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL STEP: SELECT BEST MODEL AND TEST ON X_TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The logic below demonstrates how to select the best model based on CV results.\n",
        "# For demonstration, we select the model with the highest average CV score from the best pipeline (ANOVA).\n",
        "\n",
        "# Determine which pipeline is best based on the highest average score (e.g., from MLP/RF)\n",
        "best_anova_score = max(cv_anova_scores.values())\n",
        "best_mi_score = max(cv_mi_scores.values())\n",
        "\n",
        "if best_anova_score >= best_mi_score:\n",
        "    final_features = anova_features\n",
        "    final_pipeline = \"ANOVA\"\n",
        "    best_model_name = max(cv_anova_scores, key=cv_anova_scores.get)\n",
        "    print(f\"[RESULT] Selected Pipeline: {final_pipeline} (Best Model: {best_model_name})\")\n",
        "else:\n",
        "    final_features = mi_features\n",
        "    final_pipeline = \"MI\"\n",
        "    best_model_name = max(cv_mi_scores, key=cv_mi_scores.get)\n",
        "    print(f\"[RESULT] Selected Pipeline: {final_pipeline} (Best Model: {best_model_name})\")\n",
        "\n",
        "# Final Training and Testing on the held-out X_test set\n",
        "\n",
        "# 1. SMOTE on the entire X_train_temp pool\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_final, y_train_final = sm.fit_resample(X_train_temp, y_train_temp)\n",
        "\n",
        "# 2. Scaling (Fit on SMOTEd train, Transform test)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_final = scaler.fit_transform(X_train_final)\n",
        "X_test_scaled_final = scaler.transform(X_test)\n",
        "\n",
        "# 3. Apply Final Feature Selection\n",
        "X_train_sel_final = pd.DataFrame(X_train_scaled_final, columns=X_augmented.columns)[final_features].values\n",
        "X_test_sel_final = pd.DataFrame(X_test_scaled_final, columns=X_augmented.columns)[final_features].values\n",
        "\n",
        "# 4. Train the Best Model\n",
        "final_model = cv_models[best_model_name]\n",
        "final_model.fit(X_train_sel_final, y_train_final)\n",
        "\n",
        "# 5. Final Test Evaluation\n",
        "y_pred_test_final = final_model.predict(X_test_sel_final)\n",
        "final_accuracy = accuracy_score(y_test, y_pred_test_final)\n",
        "\n",
        "print(f\"\\nFINAL TEST ACCURACY for {best_model_name} (using {final_pipeline} features): {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6znG19p8mZl",
        "outputId": "285b8582-c505-469f-c621-ef1111a1bade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Data augmented (GN). New total size: 627 samples.\n",
            "[INFO] CV Pool Size: 501 | Final Test Size: 126\n",
            "\n",
            "============================================================\n",
            "RUNNING CROSS-VALIDATION PIPELINES (Requires IMBLEARN and XGBOOST)\n",
            "============================================================\n",
            "\n",
            "--- Running ANOVA Pipeline (5 Folds) ---\n",
            "ANOVA CV Results (Mean Accuracy): {'RF': np.float64(0.8862178217821782), 'MLP': np.float64(0.9520990099009902), 'LR': np.float64(0.7345346534653465)}\n",
            "\n",
            "--- Running MI Pipeline (5 Folds) ---\n",
            "MI CV Results (Mean Accuracy): {'RF': np.float64(0.9021980198019802), 'MLP': np.float64(0.9520792079207923), 'LR': np.float64(0.7044950495049505)}\n",
            "\n",
            "============================================================\n",
            "FINAL STEP: SELECT BEST MODEL AND TEST ON X_TEST\n",
            "============================================================\n",
            "[RESULT] Selected Pipeline: ANOVA (Best Model: MLP)\n",
            "\n",
            "FINAL TEST ACCURACY for MLP (using ANOVA features): 0.9524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CM ALl models ANOVA + MI"
      ],
      "metadata": {
        "id": "9BYIRev78JCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# Note: XGBClassifier and VotingClassifier were removed from imports for simplicity but can be added back.\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "AUGMENTATION_FACTOR = 3\n",
        "NOISE_STD_DEV = 0.1\n",
        "TOP_K_FEATURES = 30\n",
        "N_SPLITS = 5 # K for K-Fold Cross-Validation\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load & clean\n",
        "# ----------------------------\n",
        "df = pd.read_csv(\"survey_data_cleaned.csv\")\n",
        "drop_cols = ['Timestamp', 'Date_Of_Birth', 'Age']\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
        "\n",
        "target_col = 'Career_Interest'\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(str)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Encode categorical features\n",
        "# ----------------------------\n",
        "X_enc = X.copy()\n",
        "for col in X_enc.columns:\n",
        "    if X_enc[col].dtype == 'object' or X_enc[col].dtype.name == 'category':\n",
        "        le = LabelEncoder()\n",
        "        X_enc[col] = le.fit_transform(X_enc[col].astype(str))\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_enc = le_target.fit_transform(y)\n",
        "\n",
        "df_enc = X_enc.copy()\n",
        "df_enc[target_col] = y_enc\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Augmentation (Gaussian Noise)\n",
        "# ----------------------------\n",
        "def create_augmented_data(df_original, factor, target_column, noise_std_dev=0.05):\n",
        "    df_list = [df_original.copy()]\n",
        "    feature_cols = df_original.columns.drop(target_column).tolist()\n",
        "\n",
        "    for _ in range(factor - 1):\n",
        "        df_new = df_original.copy()\n",
        "        for col in feature_cols:\n",
        "            noise = np.random.normal(0, noise_std_dev * df_new[col].std(), size=len(df_new))\n",
        "            df_new[col] = np.maximum(df_new[col] + noise, 0)\n",
        "        df_list.append(df_new)\n",
        "    return pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df_augmented = create_augmented_data(df_enc, AUGMENTATION_FACTOR, target_col, NOISE_STD_DEV)\n",
        "print(f\"[INFO] Data augmented (GN). New total size: {df_augmented.shape[0]} samples.\")\n",
        "\n",
        "X_augmented = df_augmented.drop(columns=[target_col])\n",
        "y_augmented = df_augmented[target_col]\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Train/Test split (80% for CV/Training, 20% for Final Test)\n",
        "# ----------------------------\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_augmented, y_augmented, test_size=0.2, stratify=y_augmented, random_state=RANDOM_STATE\n",
        ")\n",
        "X_train_temp = pd.DataFrame(X_train_temp, columns=X_augmented.columns)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Define Models for CV\n",
        "# ----------------------------\n",
        "cv_models = {\n",
        "    'RF': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=RANDOM_STATE),\n",
        "    'LR': LogisticRegression(max_iter=500, random_state=RANDOM_STATE),\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6) Pipeline 1: ANOVA + K-Fold Cross-Validation\n",
        "# ----------------------------------------------------\n",
        "def run_cv_pipeline_anova(X, y, models, n_splits=N_SPLITS):\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    cv_results = {name: [] for name in models.keys()}\n",
        "\n",
        "    print(f\"\\n--- Running ANOVA Pipeline ({n_splits} Folds) ---\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train_f, X_val_f = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_f, y_val_f = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # SMOTE, Scaling, and Feature Selection applied INSIDE the fold\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_train_sm, y_train_sm = sm.fit_resample(X_train_f, y_train_f)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "        X_val_scaled = scaler.transform(X_val_f)\n",
        "\n",
        "        F_vals, p_vals = f_classif(X_train_scaled, y_train_sm)\n",
        "        anova_df = pd.DataFrame({'Feature': X.columns, 'p_value': p_vals})\n",
        "        selected_features = anova_df[anova_df['p_value'] < 0.05]['Feature'].tolist()\n",
        "        if not selected_features: selected_features = X.columns.tolist()\n",
        "\n",
        "        X_train_sel = pd.DataFrame(X_train_scaled, columns=X.columns)[selected_features].values\n",
        "        X_val_sel = pd.DataFrame(X_val_scaled, columns=X.columns)[selected_features].values\n",
        "\n",
        "        for name, mdl in models.items():\n",
        "            mdl.fit(X_train_sel, y_train_sm)\n",
        "            y_pred = mdl.predict(X_val_sel)\n",
        "            cv_results[name].append(accuracy_score(y_val_f, y_pred))\n",
        "\n",
        "    return {name: np.mean(scores) for name, scores in cv_results.items()}, selected_features\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 7) Pipeline 2: Mutual Information + K-Fold Cross-Validation\n",
        "# ----------------------------------------------------\n",
        "def run_cv_pipeline_mi(X, y, models, n_splits=N_SPLITS, top_k=TOP_K_FEATURES):\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    cv_results = {name: [] for name in models.keys()}\n",
        "\n",
        "    print(f\"\\n--- Running MI Pipeline ({n_splits} Folds) ---\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train_f, X_val_f = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_f, y_val_f = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # SMOTE, Scaling, and Feature Selection applied INSIDE the fold\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_train_sm, y_train_sm = sm.fit_resample(X_train_f, y_train_f)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "        X_val_scaled = scaler.transform(X_val_f)\n",
        "\n",
        "        MI_scores = mutual_info_classif(X_train_scaled, y_train_sm, random_state=RANDOM_STATE)\n",
        "        mi_df = pd.DataFrame({'Feature': X.columns, 'MI_Score': MI_scores})\n",
        "        selected_features = mi_df.nlargest(top_k, 'MI_Score')['Feature'].tolist()\n",
        "        if not selected_features: selected_features = X.columns.tolist()\n",
        "\n",
        "        X_train_sel = pd.DataFrame(X_train_scaled, columns=X.columns)[selected_features].values\n",
        "        X_val_sel = pd.DataFrame(X_val_scaled, columns=X.columns)[selected_features].values\n",
        "\n",
        "        for name, mdl in models.items():\n",
        "            mdl.fit(X_train_sel, y_train_sm)\n",
        "            y_pred = mdl.predict(X_val_sel)\n",
        "            cv_results[name].append(accuracy_score(y_val_f, y_pred))\n",
        "\n",
        "    return {name: np.mean(scores) for name, scores in cv_results.items()}, selected_features\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 8) EXECUTION AND FINAL COMPARISON\n",
        "# ----------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RUNNING CROSS-VALIDATION PIPELINES (Requires IMBLEARN)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run ANOVA Pipeline\n",
        "cv_anova_scores, anova_features = run_cv_pipeline_anova(X_train_temp, y_train_temp, cv_models.copy())\n",
        "print(\"ANOVA CV Results (Mean Accuracy):\", cv_anova_scores)\n",
        "\n",
        "# Run MI Pipeline\n",
        "cv_mi_scores, mi_features = run_cv_pipeline_mi(X_train_temp, y_train_temp, cv_models.copy())\n",
        "print(\"MI CV Results (Mean Accuracy):\", cv_mi_scores)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL STEP: SELECT BEST MODEL AND TEST ON X_TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Determine the best model and pipeline based on the highest average CV score\n",
        "all_scores = {(\"ANOVA\", k): v for k, v in cv_anova_scores.items()}\n",
        "all_scores.update({(\"MI\", k): v for k, v in cv_mi_scores.items()})\n",
        "\n",
        "((final_pipeline, best_model_name), _) = max(all_scores.items(), key=lambda item: item[1])\n",
        "\n",
        "final_features = anova_features if final_pipeline == \"ANOVA\" else mi_features\n",
        "\n",
        "print(f\"[RESULT] Selected Pipeline: {final_pipeline} (Best Model: {best_model_name}, Mean CV Acc: {all_scores[(final_pipeline, best_model_name)]:.4f})\")\n",
        "\n",
        "# Final Training and Testing on the held-out X_test set\n",
        "\n",
        "# 1. SMOTE on the entire X_train_temp pool\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_final, y_train_final = sm.fit_resample(X_train_temp, y_train_temp)\n",
        "\n",
        "# 2. Scaling (Fit on SMOTEd train, Transform test)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_final = scaler.fit_transform(X_train_final)\n",
        "X_test_scaled_final = scaler.transform(X_test)\n",
        "\n",
        "# 3. Apply Final Feature Selection\n",
        "X_train_sel_final = pd.DataFrame(X_train_scaled_final, columns=X_augmented.columns)[final_features].values\n",
        "X_test_sel_final = pd.DataFrame(X_test_scaled_final, columns=X_augmented.columns)[final_features].values\n",
        "\n",
        "# 4. Train the Best Model\n",
        "final_model = cv_models[best_model_name]\n",
        "final_model.fit(X_train_sel_final, y_train_final)\n",
        "\n",
        "# 5. Final Test Evaluation\n",
        "y_pred_test_final = final_model.predict(X_test_sel_final)\n",
        "final_accuracy = accuracy_score(y_test, y_pred_test_final)\n",
        "\n",
        "print(f\"\\nFINAL TEST ACCURACY for {best_model_name} (using {final_pipeline} features): {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLvBiuVq9gFd",
        "outputId": "68f94416-6d24-4358-94f9-62db21c27e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Data augmented (GN). New total size: 627 samples.\n",
            "\n",
            "============================================================\n",
            "RUNNING CROSS-VALIDATION PIPELINES (Requires IMBLEARN)\n",
            "============================================================\n",
            "\n",
            "--- Running ANOVA Pipeline (5 Folds) ---\n",
            "ANOVA CV Results (Mean Accuracy): {'RF': np.float64(0.8941980198019802), 'MLP': np.float64(0.9541188118811881), 'LR': np.float64(0.7364356435643564)}\n",
            "\n",
            "--- Running MI Pipeline (5 Folds) ---\n",
            "MI CV Results (Mean Accuracy): {'RF': np.float64(0.9141386138613863), 'MLP': np.float64(0.95009900990099), 'LR': np.float64(0.7126138613861386)}\n",
            "\n",
            "============================================================\n",
            "FINAL STEP: SELECT BEST MODEL AND TEST ON X_TEST\n",
            "============================================================\n",
            "[RESULT] Selected Pipeline: ANOVA (Best Model: MLP, Mean CV Acc: 0.9541)\n",
            "\n",
            "FINAL TEST ACCURACY for MLP (using ANOVA features): 0.9365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC # New Model\n",
        "from sklearn.naive_bayes import GaussianNB # New Model\n",
        "from xgboost import XGBClassifier # New Model (Requires installation)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "AUGMENTATION_FACTOR = 3\n",
        "NOISE_STD_DEV = 0.1\n",
        "TOP_K_FEATURES = 30\n",
        "N_SPLITS = 5 # K for K-Fold Cross-Validation\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load & clean\n",
        "# ----------------------------\n",
        "df = pd.read_csv(\"survey_data_cleaned.csv\")\n",
        "drop_cols = ['Timestamp', 'Date_Of_Birth', 'Age']\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
        "\n",
        "target_col = 'Career_Interest'\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(str)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Encode categorical features\n",
        "# ----------------------------\n",
        "X_enc = X.copy()\n",
        "for col in X_enc.columns:\n",
        "    if X_enc[col].dtype == 'object' or X_enc[col].dtype.name == 'category':\n",
        "        le = LabelEncoder()\n",
        "        X_enc[col] = le.fit_transform(X_enc[col].astype(str))\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_enc = le_target.fit_transform(y)\n",
        "\n",
        "df_enc = X_enc.copy()\n",
        "df_enc[target_col] = y_enc\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Augmentation (Gaussian Noise)\n",
        "# ----------------------------\n",
        "def create_augmented_data(df_original, factor, target_column, noise_std_dev=0.05):\n",
        "    df_list = [df_original.copy()]\n",
        "    feature_cols = df_original.columns.drop(target_column).tolist()\n",
        "\n",
        "    for _ in range(factor - 1):\n",
        "        df_new = df_original.copy()\n",
        "        for col in feature_cols:\n",
        "            noise = np.random.normal(0, noise_std_dev * df_new[col].std(), size=len(df_new))\n",
        "            df_new[col] = np.maximum(df_new[col] + noise, 0)\n",
        "        df_list.append(df_new)\n",
        "    return pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df_augmented = create_augmented_data(df_enc, AUGMENTATION_FACTOR, target_col, NOISE_STD_DEV)\n",
        "X_augmented = df_augmented.drop(columns=[target_col])\n",
        "y_augmented = df_augmented[target_col]\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Train/Test split\n",
        "# ----------------------------\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_augmented, y_augmented, test_size=0.2, stratify=y_augmented, random_state=RANDOM_STATE\n",
        ")\n",
        "X_train_temp = pd.DataFrame(X_train_temp, columns=X_augmented.columns)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Define Models for CV (Expanded)\n",
        "# ----------------------------\n",
        "cv_models = {\n",
        "    'RF': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "    'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_STATE), # NEW\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=RANDOM_STATE),\n",
        "    'LR': LogisticRegression(max_iter=500, random_state=RANDOM_STATE),\n",
        "    'SVM': SVC(random_state=RANDOM_STATE), # NEW\n",
        "    'GNB': GaussianNB() # NEW\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6) Pipeline 1: ANOVA + K-Fold Cross-Validation\n",
        "# ----------------------------------------------------\n",
        "def run_cv_pipeline_anova(X, y, models, n_splits=N_SPLITS):\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    cv_results = {name: [] for name in models.keys()}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train_f, X_val_f = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_f, y_val_f = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # SMOTE, Scaling, and Feature Selection applied INSIDE the fold\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_train_sm, y_train_sm = sm.fit_resample(X_train_f, y_train_f)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "        X_val_scaled = scaler.transform(X_val_f)\n",
        "\n",
        "        F_vals, p_vals = f_classif(X_train_scaled, y_train_sm)\n",
        "        anova_df = pd.DataFrame({'Feature': X.columns, 'p_value': p_vals})\n",
        "        selected_features = anova_df[anova_df['p_value'] < 0.05]['Feature'].tolist()\n",
        "        if not selected_features: selected_features = X.columns.tolist()\n",
        "\n",
        "        X_train_sel = pd.DataFrame(X_train_scaled, columns=X.columns)[selected_features].values\n",
        "        X_val_sel = pd.DataFrame(X_val_scaled, columns=X.columns)[selected_features].values\n",
        "\n",
        "        for name, mdl in models.items():\n",
        "            mdl.fit(X_train_sel, y_train_sm)\n",
        "            y_pred = mdl.predict(X_val_sel)\n",
        "            cv_results[name].append(accuracy_score(y_val_f, y_pred))\n",
        "\n",
        "    return {name: np.mean(scores) for name, scores in cv_results.items()}, selected_features\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 7) Pipeline 2: Mutual Information + K-Fold Cross-Validation\n",
        "# ----------------------------------------------------\n",
        "def run_cv_pipeline_mi(X, y, models, n_splits=N_SPLITS, top_k=TOP_K_FEATURES):\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    cv_results = {name: [] for name in models.keys()}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train_f, X_val_f = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_f, y_val_f = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # SMOTE, Scaling, and Feature Selection applied INSIDE the fold\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_train_sm, y_train_sm = sm.fit_resample(X_train_f, y_train_f)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "        X_val_scaled = scaler.transform(X_val_f)\n",
        "\n",
        "        MI_scores = mutual_info_classif(X_train_scaled, y_train_sm, random_state=RANDOM_STATE)\n",
        "        mi_df = pd.DataFrame({'Feature': X.columns, 'MI_Score': MI_scores})\n",
        "        selected_features = mi_df.nlargest(top_k, 'MI_Score')['Feature'].tolist()\n",
        "        if not selected_features: selected_features = X.columns.tolist()\n",
        "\n",
        "        X_train_sel = pd.DataFrame(X_train_scaled, columns=X.columns)[selected_features].values\n",
        "        X_val_sel = pd.DataFrame(X_val_scaled, columns=X.columns)[selected_features].values\n",
        "\n",
        "        for name, mdl in models.items():\n",
        "            mdl.fit(X_train_sel, y_train_sm)\n",
        "            y_pred = mdl.predict(X_val_sel)\n",
        "            cv_results[name].append(accuracy_score(y_val_f, y_pred))\n",
        "\n",
        "    return {name: np.mean(scores) for name, scores in cv_results.items()}, selected_features\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 8) EXECUTION AND FINAL COMPARISON\n",
        "# ----------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RUNNING CROSS-VALIDATION PIPELINES (Requires IMBLEARN and XGBOOST)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run ANOVA Pipeline\n",
        "cv_anova_scores, anova_features = run_cv_pipeline_anova(X_train_temp, y_train_temp, cv_models.copy())\n",
        "print(\"ANOVA CV Results (Mean Accuracy):\", cv_anova_scores)\n",
        "\n",
        "# Run MI Pipeline\n",
        "cv_mi_scores, mi_features = run_cv_pipeline_mi(X_train_temp, y_train_temp, cv_models.copy())\n",
        "print(\"MI CV Results (Mean Accuracy):\", cv_mi_scores)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL STEP: SELECT BEST MODEL AND TEST ON X_TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Determine the best model and pipeline based on the highest average CV score\n",
        "all_scores = {(\"ANOVA\", k): v for k, v in cv_anova_scores.items()}\n",
        "all_scores.update({(\"MI\", k): v for k, v in cv_mi_scores.items()})\n",
        "\n",
        "((final_pipeline, best_model_name), _) = max(all_scores.items(), key=lambda item: item[1])\n",
        "\n",
        "final_features = anova_features if final_pipeline == \"ANOVA\" else mi_features\n",
        "\n",
        "print(f\"[RESULT] Selected Pipeline: {final_pipeline} (Best Model: {best_model_name}, Mean CV Acc: {all_scores[(final_pipeline, best_model_name)]:.4f})\")\n",
        "\n",
        "# Final Training and Testing on the held-out X_test set\n",
        "\n",
        "# 1. SMOTE on the entire X_train_temp pool\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_final, y_train_final = sm.fit_resample(X_train_temp, y_train_temp)\n",
        "\n",
        "# 2. Scaling (Fit on SMOTEd train, Transform test)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_final = scaler.fit_transform(X_train_final)\n",
        "X_test_scaled_final = scaler.transform(X_test)\n",
        "\n",
        "# 3. Apply Final Feature Selection\n",
        "X_train_sel_final = pd.DataFrame(X_train_scaled_final, columns=X_augmented.columns)[final_features].values\n",
        "X_test_sel_final = pd.DataFrame(X_test_scaled_final, columns=X_augmented.columns)[final_features].values\n",
        "\n",
        "# 4. Train the Best Model\n",
        "final_model = cv_models[best_model_name]\n",
        "final_model.fit(X_train_sel_final, y_train_final)\n",
        "\n",
        "# 5. Final Test Evaluation\n",
        "y_pred_test_final = final_model.predict(X_test_sel_final)\n",
        "final_accuracy = accuracy_score(y_test, y_pred_test_final)\n",
        "\n",
        "print(f\"\\nFINAL TEST ACCURACY for {best_model_name} (using {final_pipeline} features): {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSXGfKT4-e1N",
        "outputId": "04a2ab2c-4111-4ff5-9006-4f43cb1b4e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RUNNING CROSS-VALIDATION PIPELINES (Requires IMBLEARN and XGBOOST)\n",
            "============================================================\n",
            "ANOVA CV Results (Mean Accuracy): {'RF': np.float64(0.8681782178217821), 'XGB': np.float64(0.8482178217821783), 'MLP': np.float64(0.9541188118811881), 'LR': np.float64(0.7285544554455445), 'SVM': np.float64(0.8961980198019802), 'GNB': np.float64(0.5427920792079207)}\n",
            "MI CV Results (Mean Accuracy): {'RF': np.float64(0.8981386138613863), 'XGB': np.float64(0.8701980198019802), 'MLP': np.float64(0.9540990099009902), 'LR': np.float64(0.7045940594059406), 'SVM': np.float64(0.8981980198019801), 'GNB': np.float64(0.560950495049505)}\n",
            "\n",
            "============================================================\n",
            "FINAL STEP: SELECT BEST MODEL AND TEST ON X_TEST\n",
            "============================================================\n",
            "[RESULT] Selected Pipeline: ANOVA (Best Model: MLP, Mean CV Acc: 0.9541)\n",
            "\n",
            "FINAL TEST ACCURACY for MLP (using ANOVA features): 0.9524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open"
      ],
      "metadata": {
        "id": "R7l5lVl2z653"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "open with mi"
      ],
      "metadata": {
        "id": "n3gkF5nYzSUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import mutual_info_classif # ADDED for MI\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "AUGMENTATION_FACTOR = 2\n",
        "NOISE_STD_DEV = 1\n",
        "TOP_K_FEATURES = 8 # Defined a value for MI selection\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load & clean\n",
        "# ----------------------------\n",
        "df = pd.read_csv(\"Student Engagement Level-Multiclass.csv\")\n",
        "drop_cols = ['Student ID']\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
        "\n",
        "target_col = 'Engagement Level'\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(str)\n",
        "\n",
        "print(\"[INFO] Original shape:\", df.shape)\n",
        "print(\"[INFO] Original class counts:\\n\", y.value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Encode categorical features\n",
        "# ----------------------------\n",
        "X_enc = X.copy()\n",
        "encoders = {}\n",
        "for col in X_enc.columns:\n",
        "    if X_enc[col].dtype == 'object' or X_enc[col].dtype.name == 'category':\n",
        "        le = LabelEncoder()\n",
        "        X_enc[col] = le.fit_transform(X_enc[col].astype(str))\n",
        "        encoders[col] = le\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_enc = le_target.fit_transform(y)\n",
        "\n",
        "df_enc = X_enc.copy()\n",
        "df_enc[target_col] = y_enc\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Augmentation (Gaussian Noise)\n",
        "# ----------------------------\n",
        "def create_augmented_data(df_original, factor, target_column, noise_std_dev=0.05):\n",
        "    df_list = [df_original.copy()]\n",
        "    feature_cols = df_original.columns.drop(target_column).tolist()\n",
        "\n",
        "    for _ in range(factor - 1):\n",
        "        df_new = df_original.copy()\n",
        "        for col in feature_cols:\n",
        "            noise = np.random.normal(0, noise_std_dev * df_new[col].std(), size=len(df_new))\n",
        "            df_new[col] = np.maximum(df_new[col] + noise, 0)\n",
        "        df_list.append(df_new)\n",
        "\n",
        "    return pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df_augmented = create_augmented_data(df_enc, AUGMENTATION_FACTOR, target_col, NOISE_STD_DEV)\n",
        "print(f\"[INFO] Data augmented (GN). New total size: {df_augmented.shape[0]} samples.\")\n",
        "\n",
        "X_augmented = df_augmented.drop(columns=[target_col])\n",
        "y_augmented = df_augmented[target_col]\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Train/Validation/Test split (60/20/20)\n",
        "# ----------------------------\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_augmented, y_augmented, test_size=0.2, stratify=y_augmented, random_state=RANDOM_STATE\n",
        ")\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.25, stratify=y_train_temp, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Train Size: {len(X_train)} | Validation Size: {len(X_valid)} | Test Size: {len(X_test)}\")\n",
        "print(\"[INFO] Train class dist BEFORE SMOTE:\\n\", pd.Series(y_train).value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5) SMOTE on training data ONLY\n",
        "# ----------------------------\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"[INFO] Train class dist AFTER SMOTE:\\n\", pd.Series(y_train_sm).value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Scaling\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Mutual Information (MI) Feature Selection\n",
        "# ----------------------------\n",
        "print(\"\\n>>> [7/9] MUTUAL INFORMATION FEATURE SELECTION...\")\n",
        "# Calculate MI scores on the scaled, SMOTEd training data\n",
        "MI_scores = mutual_info_classif(X_train_scaled, y_train_sm, random_state=RANDOM_STATE)\n",
        "mi_df = pd.DataFrame({'Feature': X_train.columns, 'MI_Score': MI_scores})\n",
        "\n",
        "# Select top K features based on MI score\n",
        "selected_features = mi_df.nlargest(TOP_K_FEATURES, 'MI_Score')['Feature'].tolist()\n",
        "\n",
        "if len(selected_features) == 0:\n",
        "    print(\"[WARNING] MI selected 0 features. Using all features.\")\n",
        "    selected_features = X_train.columns.tolist()\n",
        "\n",
        "# Transform all datasets to selected features\n",
        "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_valid_df = pd.DataFrame(X_valid_scaled, columns=X_valid.columns)\n",
        "X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "X_train_sel = X_train_df[selected_features].values\n",
        "X_valid_sel = X_valid_df[selected_features].values\n",
        "X_test_sel = X_test_df[selected_features].values\n",
        "\n",
        "print(f\"[INFO] MI selected {len(selected_features)} features: {selected_features}\")\n",
        "print(\"[INFO] Numbers of MI selected features:\", len(selected_features), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 8) Train models\n",
        "# -------------------\n",
        "models = {\n",
        "    'RF': RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
        "    'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_STATE),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(128,64), max_iter=300, random_state=RANDOM_STATE),\n",
        "    'LR': LogisticRegression(max_iter=500, random_state=RANDOM_STATE),\n",
        "    'SVM': SVC(probability=True, random_state=RANDOM_STATE),\n",
        "    'MLR': LinearRegression()\n",
        "}\n",
        "\n",
        "trained = {}\n",
        "for name, mdl in models.items():\n",
        "    print(f\"[INFO] Training {name} ...\")\n",
        "    mdl.fit(X_train_sel, y_train_sm)\n",
        "    trained[name] = mdl\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('RF', trained['RF']), ('XGB', trained['XGB'])],\n",
        "    voting='soft'\n",
        ")\n",
        "ensemble.fit(X_train_sel, y_train_sm)\n",
        "trained['ENSEMBLE'] = ensemble\n",
        "\n",
        "# ----------------------------\n",
        "# 9) Predict & evaluate\n",
        "# ----------------------------\n",
        "def predict_model(name, model, X):\n",
        "    if name == 'MLR':\n",
        "        y_hat = np.rint(model.predict(X)).astype(int)\n",
        "        y_hat = np.clip(y_hat, y_enc.min(), y_enc.max())\n",
        "        return y_hat\n",
        "    return model.predict(X)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\">>> EVALUATION: TRAINING, VALIDATION & TEST SETS <<<\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 9.1 Accuracy Comparison\n",
        "print(\"\\n--- ACCURACY COMPARISON with MI(Training vs. Validation vs. Test) ---\")\n",
        "for name, mdl in trained.items():\n",
        "    y_pred_train = predict_model(name, mdl, X_train_sel)\n",
        "    acc_train = accuracy_score(y_train_sm, y_pred_train)\n",
        "\n",
        "    y_pred_valid = predict_model(name, mdl, X_valid_sel)\n",
        "    acc_valid = accuracy_score(y_valid, y_pred_valid)\n",
        "\n",
        "    y_pred_test = predict_model(name, mdl, X_test_sel)\n",
        "    acc_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"   {name} Training: {acc_train:.4f} | Validation: {acc_valid:.4f} | Test: {acc_test:.4f}\")\n",
        "\n",
        "# 9.2 Full Classification Report for Test Set\n",
        "print(\"\\n--- FINAL TEST SET RESULTS (Unseen Data) ---\")\n",
        "for name, mdl in trained.items():\n",
        "    y_pred = predict_model(name, mdl, X_test_sel)\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    try:\n",
        "        print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n",
        "    except ValueError:\n",
        "        print(f\"Classification Report failed for {name} due to class mismatch. Showing only Accuracy.\")\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njZOZiCAz4hp",
        "outputId": "441857fa-39a4-48a8-ab39-2658cee7f24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Original shape: (486, 13)\n",
            "[INFO] Original class counts:\n",
            " Engagement Level\n",
            "H    258\n",
            "M    214\n",
            "L     14\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] Data augmented (GN). New total size: 972 samples.\n",
            "[INFO] Train Size: 582 | Validation Size: 195 | Test Size: 195\n",
            "[INFO] Train class dist BEFORE SMOTE:\n",
            " Engagement Level\n",
            "0    309\n",
            "2    256\n",
            "1     17\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] Train class dist AFTER SMOTE:\n",
            " Engagement Level\n",
            "2    309\n",
            "0    309\n",
            "1    309\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "\n",
            ">>> [7/9] MUTUAL INFORMATION FEATURE SELECTION...\n",
            "[INFO] MI selected 8 features: ['Assignment 1 duration to submit (in hours)', 'Average time to submit assignment (in hours)', 'Assignment 3 duration to submit (in hours)', 'Assignment 2 duration to submit (in hours)', 'Assignment 1 lateness indicator', 'Assignment 3 lateness indicator', '# Logins', '# Content Reads']\n",
            "[INFO] Numbers of MI selected features: 8 \n",
            "\n",
            "[INFO] Training RF ...\n",
            "[INFO] Training XGB ...\n",
            "[INFO] Training MLP ...\n",
            "[INFO] Training LR ...\n",
            "[INFO] Training SVM ...\n",
            "[INFO] Training MLR ...\n",
            "\n",
            "============================================================\n",
            ">>> EVALUATION: TRAINING, VALIDATION & TEST SETS <<<\n",
            "============================================================\n",
            "\n",
            "--- ACCURACY COMPARISON with MI(Training vs. Validation vs. Test) ---\n",
            "   RF Training: 1.0000 | Validation: 0.9590 | Test: 0.9436\n",
            "   XGB Training: 1.0000 | Validation: 0.9692 | Test: 0.9436\n",
            "   MLP Training: 1.0000 | Validation: 0.9692 | Test: 0.9590\n",
            "   LR Training: 0.9752 | Validation: 0.9590 | Test: 0.9590\n",
            "   SVM Training: 0.9838 | Validation: 0.9538 | Test: 0.9538\n",
            "   MLR Training: 0.8101 | Validation: 0.7436 | Test: 0.7026\n",
            "   ENSEMBLE Training: 1.0000 | Validation: 0.9692 | Test: 0.9436\n",
            "\n",
            "--- FINAL TEST SET RESULTS (Unseen Data) ---\n",
            "\n",
            "===== RF =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.96      0.94      0.95       103\n",
            "           L       0.86      1.00      0.92         6\n",
            "           M       0.93      0.94      0.94        86\n",
            "\n",
            "    accuracy                           0.94       195\n",
            "   macro avg       0.92      0.96      0.94       195\n",
            "weighted avg       0.94      0.94      0.94       195\n",
            "\n",
            "Test Accuracy: 0.9435897435897436\n",
            "\n",
            "===== XGB =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.96      0.94      0.95       103\n",
            "           L       0.86      1.00      0.92         6\n",
            "           M       0.93      0.94      0.94        86\n",
            "\n",
            "    accuracy                           0.94       195\n",
            "   macro avg       0.92      0.96      0.94       195\n",
            "weighted avg       0.94      0.94      0.94       195\n",
            "\n",
            "Test Accuracy: 0.9435897435897436\n",
            "\n",
            "===== MLP =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.97      0.95      0.96       103\n",
            "           L       1.00      1.00      1.00         6\n",
            "           M       0.94      0.97      0.95        86\n",
            "\n",
            "    accuracy                           0.96       195\n",
            "   macro avg       0.97      0.97      0.97       195\n",
            "weighted avg       0.96      0.96      0.96       195\n",
            "\n",
            "Test Accuracy: 0.958974358974359\n",
            "\n",
            "===== LR =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.97      0.95      0.96       103\n",
            "           L       1.00      1.00      1.00         6\n",
            "           M       0.94      0.97      0.95        86\n",
            "\n",
            "    accuracy                           0.96       195\n",
            "   macro avg       0.97      0.97      0.97       195\n",
            "weighted avg       0.96      0.96      0.96       195\n",
            "\n",
            "Test Accuracy: 0.958974358974359\n",
            "\n",
            "===== SVM =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.98      0.95      0.97       103\n",
            "           L       1.00      0.67      0.80         6\n",
            "           M       0.92      0.98      0.95        86\n",
            "\n",
            "    accuracy                           0.95       195\n",
            "   macro avg       0.97      0.86      0.90       195\n",
            "weighted avg       0.96      0.95      0.95       195\n",
            "\n",
            "Test Accuracy: 0.9538461538461539\n",
            "\n",
            "===== MLR =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.97      0.71      0.82       103\n",
            "           L       0.05      0.50      0.09         6\n",
            "           M       0.98      0.71      0.82        86\n",
            "\n",
            "    accuracy                           0.70       195\n",
            "   macro avg       0.67      0.64      0.58       195\n",
            "weighted avg       0.95      0.70      0.80       195\n",
            "\n",
            "Test Accuracy: 0.7025641025641025\n",
            "\n",
            "===== ENSEMBLE =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.96      0.94      0.95       103\n",
            "           L       0.86      1.00      0.92         6\n",
            "           M       0.93      0.94      0.94        86\n",
            "\n",
            "    accuracy                           0.94       195\n",
            "   macro avg       0.92      0.96      0.94       195\n",
            "weighted avg       0.94      0.94      0.94       195\n",
            "\n",
            "Test Accuracy: 0.9435897435897436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "open with anova"
      ],
      "metadata": {
        "id": "i6aqhogvzOZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "# NOTE: XGBClassifier import may fail if not installed\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "# NOTE: SMOTE import will fail if 'imblearn' is not installed\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "AUGMENTATION_FACTOR = 2\n",
        "NOISE_STD_DEV = 1\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load & clean\n",
        "# ----------------------------\n",
        "df = pd.read_csv(\"Student Engagement Level-Multiclass.csv\")\n",
        "drop_cols = ['Student ID']\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
        "\n",
        "target_col = 'Engagement Level'\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(str)\n",
        "\n",
        "print(\"[INFO] Original shape:\", df.shape)\n",
        "print(\"[INFO] Original class counts:\\n\", y.value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Encode categorical features\n",
        "# ----------------------------\n",
        "X_enc = X.copy()\n",
        "encoders = {}\n",
        "for col in X_enc.columns:\n",
        "    if X_enc[col].dtype == 'object' or X_enc[col].dtype.name == 'category':\n",
        "        le = LabelEncoder()\n",
        "        X_enc[col] = le.fit_transform(X_enc[col].astype(str))\n",
        "        encoders[col] = le\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_enc = le_target.fit_transform(y)\n",
        "\n",
        "df_enc = X_enc.copy()\n",
        "df_enc[target_col] = y_enc\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Augmentation (Gaussian Noise)\n",
        "# ----------------------------\n",
        "def create_augmented_data(df_original, factor, target_column, noise_std_dev=0.05):\n",
        "    df_list = [df_original.copy()]\n",
        "    feature_cols = df_original.columns.drop(target_column).tolist()\n",
        "\n",
        "    for _ in range(factor - 1):\n",
        "        df_new = df_original.copy()\n",
        "        for col in feature_cols:\n",
        "            noise = np.random.normal(0, noise_std_dev * df_new[col].std(), size=len(df_new))\n",
        "            df_new[col] = np.maximum(df_new[col] + noise, 0)\n",
        "        df_list.append(df_new)\n",
        "\n",
        "    return pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "df_augmented = create_augmented_data(df_enc, AUGMENTATION_FACTOR, target_col, NOISE_STD_DEV)\n",
        "print(f\"[INFO] Data augmented (GN). New total size: {df_augmented.shape[0]} samples.\")\n",
        "\n",
        "X_augmented = df_augmented.drop(columns=[target_col])\n",
        "y_augmented = df_augmented[target_col]\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Train/Validation/Test split (60/20/20)\n",
        "# ----------------------------\n",
        "# 1. Split into Train_Temp (80%) and Test (20%)\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_augmented, y_augmented, test_size=0.2, stratify=y_augmented, random_state=RANDOM_STATE\n",
        ")\n",
        "# 2. Split Train_Temp (80%) into Train (75% of 80% = 60%) and Validation (25% of 80% = 20%)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.25, stratify=y_train_temp, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Train Size: {len(X_train)} | Validation Size: {len(X_valid)} | Test Size: {len(X_test)}\")\n",
        "print(\"[INFO] Train class dist BEFORE SMOTE:\\n\", pd.Series(y_train).value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5) SMOTE on training data ONLY\n",
        "# ----------------------------\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "# SMOTE is applied only to the 60% training set\n",
        "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"[INFO] Train class dist AFTER SMOTE:\\n\", pd.Series(y_train_sm).value_counts(), \"\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Scaling\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_sm)\n",
        "X_valid_scaled = scaler.transform(X_valid) # Transform Validation set\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# 7) ANOVA\n",
        "# ----------------------------\n",
        "F_vals, p_vals = f_classif(X_train_scaled, y_train_sm)\n",
        "anova_df = pd.DataFrame({'Feature': X_train.columns, 'F_value': F_vals, 'p_value': p_vals})\n",
        "selected_features = anova_df[anova_df['p_value'] < 0.05]['Feature'].tolist()\n",
        "if len(selected_features) == 0:\n",
        "    selected_features = X_train.columns.tolist()\n",
        "\n",
        "# Apply feature selection to all three sets\n",
        "X_train_sel = pd.DataFrame(X_train_scaled, columns=X_train.columns)[selected_features].values\n",
        "X_valid_sel = pd.DataFrame(X_valid_scaled, columns=X_valid.columns)[selected_features].values\n",
        "X_test_sel = pd.DataFrame(X_test_scaled, columns=X_test.columns)[selected_features].values\n",
        "print(\"[INFO] ANOVA selected features:\", selected_features, \"\\n\")\n",
        "print(\"[INFO] Numbers of ANOVA selected features:\", len(selected_features), \"\\n\")\n",
        "# ----------------------------\n",
        "# 8) Train models\n",
        "# -------------------\n",
        "models = {\n",
        "    'RF': RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
        "    'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_STATE),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(128,64), max_iter=300, random_state=RANDOM_STATE),\n",
        "    'LR': LogisticRegression(max_iter=500, random_state=RANDOM_STATE),\n",
        "    'SVM': SVC(probability=True, random_state=RANDOM_STATE),\n",
        "    'MLR': LinearRegression()\n",
        "}\n",
        "\n",
        "trained = {}\n",
        "for name, mdl in models.items():\n",
        "    print(f\"[INFO] Training {name} ...\")\n",
        "    mdl.fit(X_train_sel, y_train_sm)\n",
        "    trained[name] = mdl\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('RF', trained['RF']), ('XGB', trained['XGB'])],\n",
        "    voting='soft'\n",
        ")\n",
        "ensemble.fit(X_train_sel, y_train_sm)\n",
        "trained['ENSEMBLE'] = ensemble\n",
        "\n",
        "# ----------------------------\n",
        "# 9) Predict & evaluate\n",
        "# ----------------------------\n",
        "def predict_model(name, model, X):\n",
        "    if name == 'MLR':\n",
        "        # LinearRegression for multiclass is an approximation, rounding results\n",
        "        y_hat = np.rint(model.predict(X)).astype(int)\n",
        "        # Clip to the range of target classes\n",
        "        y_hat = np.clip(y_hat, y_enc.min(), y_enc.max())\n",
        "        return y_hat\n",
        "    return model.predict(X)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\">>> EVALUATION: TRAINING, VALIDATION & TEST SETS <<<\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 9.1 Accuracy Comparison\n",
        "print(\"\\n--- ACCURACY COMPARISON with ANOVA(Training vs. Validation vs. Test) ---\")\n",
        "for name, mdl in trained.items():\n",
        "    # Training Prediction (on augmented/SMOTEd data)\n",
        "    y_pred_train = predict_model(name, mdl, X_train_sel)\n",
        "    acc_train = accuracy_score(y_train_sm, y_pred_train)\n",
        "\n",
        "    # Validation Prediction (on unseen, non-SMOTEd data)\n",
        "    y_pred_valid = predict_model(name, mdl, X_valid_sel)\n",
        "    acc_valid = accuracy_score(y_valid, y_pred_valid)\n",
        "\n",
        "    # Test Prediction (on unseen, non-SMOTEd data)\n",
        "    y_pred_test = predict_model(name, mdl, X_test_sel)\n",
        "    acc_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"   {name} Training: {acc_train:.4f} | Validation: {acc_valid:.4f} | Test: {acc_test:.4f}\")\n",
        "\n",
        "# 9.2 Full Classification Report for Test Set\n",
        "print(\"\\n--- FINAL TEST SET RESULTS (Unseen Data) ---\")\n",
        "for name, mdl in trained.items():\n",
        "    y_pred = predict_model(name, mdl, X_test_sel)\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    # Note: If LinearRegression (MMR) is used, classes might not match perfectly\n",
        "    try:\n",
        "        print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n",
        "    except ValueError:\n",
        "        print(f\"Classification Report failed for {name} due to class mismatch. Showing only Accuracy.\")\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf_r0Dio5wgO",
        "outputId": "1efffa5e-2a57-47ad-e369-1387dd7fa9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Original shape: (486, 13)\n",
            "[INFO] Original class counts:\n",
            " Engagement Level\n",
            "H    258\n",
            "M    214\n",
            "L     14\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] Data augmented (GN). New total size: 972 samples.\n",
            "[INFO] Train Size: 582 | Validation Size: 195 | Test Size: 195\n",
            "[INFO] Train class dist BEFORE SMOTE:\n",
            " Engagement Level\n",
            "0    309\n",
            "2    256\n",
            "1     17\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] Train class dist AFTER SMOTE:\n",
            " Engagement Level\n",
            "2    309\n",
            "0    309\n",
            "1    309\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "[INFO] ANOVA selected features: ['# Logins', '# Content Reads', '# Forum Reads', '# Forum Posts', '# Quiz Reviews before submission', 'Assignment 1 lateness indicator', 'Assignment 2 lateness indicator', 'Assignment 3 lateness indicator', 'Assignment 1 duration to submit (in hours)', 'Assignment 2 duration to submit (in hours)', 'Assignment 3 duration to submit (in hours)', 'Average time to submit assignment (in hours)'] \n",
            "\n",
            "[INFO] Numbers of ANOVA selected features: 12 \n",
            "\n",
            "[INFO] Training RF ...\n",
            "[INFO] Training XGB ...\n",
            "[INFO] Training MLP ...\n",
            "[INFO] Training LR ...\n",
            "[INFO] Training SVM ...\n",
            "[INFO] Training MLR ...\n",
            "\n",
            "============================================================\n",
            ">>> EVALUATION: TRAINING, VALIDATION & TEST SETS <<<\n",
            "============================================================\n",
            "\n",
            "--- ACCURACY COMPARISON with ANOVA(Training vs. Validation vs. Test) ---\n",
            "   RF Training: 1.0000 | Validation: 0.9385 | Test: 0.9538\n",
            "   XGB Training: 1.0000 | Validation: 0.9385 | Test: 0.9436\n",
            "   MLP Training: 1.0000 | Validation: 0.9436 | Test: 0.9231\n",
            "   LR Training: 0.9698 | Validation: 0.9333 | Test: 0.9538\n",
            "   SVM Training: 0.9817 | Validation: 0.9590 | Test: 0.9487\n",
            "   MLR Training: 0.8069 | Validation: 0.7436 | Test: 0.7333\n",
            "   ENSEMBLE Training: 1.0000 | Validation: 0.9385 | Test: 0.9487\n",
            "\n",
            "--- FINAL TEST SET RESULTS (Unseen Data) ---\n",
            "\n",
            "===== RF =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.95      0.96      0.96       103\n",
            "           L       1.00      1.00      1.00         6\n",
            "           M       0.95      0.94      0.95        86\n",
            "\n",
            "    accuracy                           0.95       195\n",
            "   macro avg       0.97      0.97      0.97       195\n",
            "weighted avg       0.95      0.95      0.95       195\n",
            "\n",
            "Test Accuracy: 0.9538461538461539\n",
            "\n",
            "===== XGB =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.94      0.95      0.95       103\n",
            "           L       1.00      1.00      1.00         6\n",
            "           M       0.94      0.93      0.94        86\n",
            "\n",
            "    accuracy                           0.94       195\n",
            "   macro avg       0.96      0.96      0.96       195\n",
            "weighted avg       0.94      0.94      0.94       195\n",
            "\n",
            "Test Accuracy: 0.9435897435897436\n",
            "\n",
            "===== MLP =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.93      0.92      0.93       103\n",
            "           L       1.00      0.83      0.91         6\n",
            "           M       0.91      0.93      0.92        86\n",
            "\n",
            "    accuracy                           0.92       195\n",
            "   macro avg       0.95      0.90      0.92       195\n",
            "weighted avg       0.92      0.92      0.92       195\n",
            "\n",
            "Test Accuracy: 0.9230769230769231\n",
            "\n",
            "===== LR =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.95      0.96      0.96       103\n",
            "           L       1.00      1.00      1.00         6\n",
            "           M       0.95      0.94      0.95        86\n",
            "\n",
            "    accuracy                           0.95       195\n",
            "   macro avg       0.97      0.97      0.97       195\n",
            "weighted avg       0.95      0.95      0.95       195\n",
            "\n",
            "Test Accuracy: 0.9538461538461539\n",
            "\n",
            "===== SVM =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.96      0.95      0.96       103\n",
            "           L       1.00      0.83      0.91         6\n",
            "           M       0.93      0.95      0.94        86\n",
            "\n",
            "    accuracy                           0.95       195\n",
            "   macro avg       0.96      0.91      0.94       195\n",
            "weighted avg       0.95      0.95      0.95       195\n",
            "\n",
            "Test Accuracy: 0.9487179487179487\n",
            "\n",
            "===== MLR =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.96      0.73      0.83       103\n",
            "           L       0.06      0.50      0.11         6\n",
            "           M       0.98      0.76      0.86        86\n",
            "\n",
            "    accuracy                           0.73       195\n",
            "   macro avg       0.67      0.66      0.60       195\n",
            "weighted avg       0.94      0.73      0.82       195\n",
            "\n",
            "Test Accuracy: 0.7333333333333333\n",
            "\n",
            "===== ENSEMBLE =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           H       0.95      0.95      0.95       103\n",
            "           L       1.00      1.00      1.00         6\n",
            "           M       0.94      0.94      0.94        86\n",
            "\n",
            "    accuracy                           0.95       195\n",
            "   macro avg       0.96      0.96      0.96       195\n",
            "weighted avg       0.95      0.95      0.95       195\n",
            "\n",
            "Test Accuracy: 0.9487179487179487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htzirE8Ox6-l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}